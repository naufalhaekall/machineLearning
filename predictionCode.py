# -*- coding: utf-8 -*-
"""Bismillah Final Thesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16dD7W9Amt4jm3tgM0d-7kadSLWJZ7tw8

# Connect Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Noise Reduction"""

!pip install noisereduce

import os
import librosa
import noisereduce as nr
import soundfile as sf

def create_noise_profile(audio_path, start_sec, end_sec):
    audio_data, sample_rate = librosa.load(audio_path, sr=None)

    # Extract noise profile
    start_sample = int(start_sec * sample_rate)
    end_sample = int(end_sec * sample_rate)
    noise_data = audio_data[start_sample:end_sample]

    return noise_data, sample_rate

def apply_noise_reduction(audio_path, noise_data, output_path):
    audio_data, sample_rate = librosa.load(audio_path, sr=None)

    # Reduce noise with noise profile
    reduced_noise = nr.reduce_noise(y=audio_data, y_noise=noise_data, sr=sample_rate)

    # Ensure duration of reduced noise same as original
    if len(reduced_noise) != len(audio_data):
        print("Warning: Duration mismatch after noise reduction. Adjusting to original duration.")
        reduced_noise = librosa.util.fix_length(reduced_noise, len(audio_data))

    # Save the noise-reduced audio
    sf.write(output_path, reduced_noise, sample_rate)

def process_folder(folder_path, output_folder, noise_start_sec, noise_end_sec):
    # Ensure the output folder exists
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Loop through each file in the folder
    for filename in os.listdir(folder_path):
        if filename.endswith('.wav'):  # Check for WAV files
            audio_path = os.path.join(folder_path, filename)
            output_path = os.path.join(output_folder, f'nr_{filename}')

            # Create a noise profile
            noise_data, sample_rate = create_noise_profile(audio_path, noise_start_sec, noise_end_sec)

            # Apply noise reduction
            apply_noise_reduction(audio_path, noise_data, output_path)

# Example usage
folder_path = '/content/Berhenti'
output_folder = '/content/Reduction'
process_folder(folder_path, output_folder, 0, 5)  # Noise profile in first 5 second

"""### Visualization"""

import librosa
import librosa.display
import matplotlib.pyplot as plt

def visualize_audio_comparison(clean_audio_path, noisy_audio_path, target_sr=16000):
    # Load the clean and noisy audio files with their original sample rates
    clean_audio, sr_clean = librosa.load(clean_audio_path, sr=None)
    noisy_audio, sr_noisy = librosa.load(noisy_audio_path, sr=None)

    # Resample both audios to the target sample rate if their sample rates differ
    if sr_clean != target_sr:
        clean_audio = librosa.resample(clean_audio, orig_sr=sr_clean, target_sr=target_sr)
        sr_clean = target_sr
    if sr_noisy != target_sr:
        noisy_audio = librosa.resample(noisy_audio, orig_sr=sr_noisy, target_sr=target_sr)
        sr_noisy = target_sr

    # Create a figure with two subplots
    plt.figure(figsize=(12, 8))

    # Plot the clean audio
    plt.subplot(2, 1, 1)
    librosa.display.waveshow(clean_audio, sr=sr_clean, color='b')
    plt.title('Navigation Audio')
    plt.xlabel('Time (s)')
    plt.ylabel('Amplitude')

    # Plot the noisy audio
    plt.subplot(2, 1, 2)
    librosa.display.waveshow(noisy_audio, sr=sr_noisy, color='r')
    plt.title('Reduction Navigation Audio')
    plt.xlabel('Time (s)')
    plt.ylabel('Amplitude')

    # Display the plots
    plt.tight_layout()
    plt.show()

noise_audio_path = '/content/Test Maju 3.wav'
reduction_audio_path = '/content/Processed Test Maju 3.wav'

visualize_audio_comparison(noise_audio_path, reduction_audio_path)

"""# SNR Analysis"""

import librosa
import numpy as np
import matplotlib.pyplot as plt

# Calculate SNR
def calculate_snr(signal, noise):
    signal_power = np.mean(signal ** 2)
    noise_power = np.mean(noise ** 2)
    snr = 10 * np.log10(signal_power / noise_power)
    return snr

# Define signal and noise segment
def separate_signal_and_noise(audio_data, threshold_ratio=0.2):
    rms_amplitude = np.sqrt(np.mean(audio_data ** 2))
    noise_threshold = rms_amplitude * threshold_ratio
    noise_segment = audio_data[np.abs(audio_data) < noise_threshold]
    signal_segment = audio_data[np.abs(audio_data) >= noise_threshold]

    return signal_segment, noise_segment

# Plot waveforms
def plot_waveforms_side_by_side(audio_data_1, audio_data_2, sample_rate_1, sample_rate_2, snr_value_1, snr_value_2):
    time_1 = np.arange(len(audio_data_1)) / sample_rate_1
    time_2 = np.arange(len(audio_data_2)) / sample_rate_2

    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    plt.plot(time_1, audio_data_1, color='b')
    plt.xlabel("Time (s)")
    plt.ylabel("Amplitude")
    plt.title(f"Before Noise Reduction\nSNR: {snr_value_1:.2f} dB")

    plt.subplot(1, 2, 2)
    plt.plot(time_2, audio_data_2, color='r')
    plt.xlabel("Time (s)")
    plt.ylabel("Amplitude")
    plt.title(f"After Noise Reduction\nSNR: {snr_value_2:.2f} dB")

    plt.tight_layout()
    plt.show()

import librosa
import numpy as np

def main():
    audio_file_1 = "/content/Test Maju 3.wav"
    audio_file_2 = "/content/Processed Test Maju 3.wav"

    audio_data_1, sample_rate_1 = librosa.load(audio_file_1, sr=None)
    audio_data_2, sample_rate_2 = librosa.load(audio_file_2, sr=None)

    # Separate signal and noise
    signal_1, noise_1 = separate_signal_and_noise(audio_data_1)
    signal_2, noise_2 = separate_signal_and_noise(audio_data_2)

    # Calculate SNR
    snr_value_1 = calculate_snr(signal_1, noise_1)
    snr_value_2 = calculate_snr(signal_2, noise_2)

    # Plot waveforms with SNR values
    plot_waveforms_side_by_side(audio_data_1, audio_data_2, sample_rate_1, sample_rate_2, snr_value_1, snr_value_2)

if __name__ == "__main__":
    main()

"""# Dataset Preparation"""

import os
import numpy as np
from sklearn.model_selection import train_test_split

# Navigation Dataset and Class Name
data_dir = '/content/With Noise Reduction'
class_names = ['Maju', 'Mundur', 'Kanan', 'Kiri', 'Berhenti']

file_paths = []
labels = []

# Collect file paths and labels
for idx, class_name in enumerate(class_names):
    class_folder = os.path.join(data_dir, class_name)
    for filename in os.listdir(class_folder):
        if filename.endswith('.wav'):
            file_path = os.path.join(class_folder, filename)
            file_paths.append(file_path)
            labels.append(idx)

file_paths = np.array(file_paths)
labels = np.array(labels)

# Dataset Debugging
n = 100

for i in range(0, len(file_paths), n):
    label = labels[i]
    class_name = class_names[label] if label < len(class_names) else "Unknown"
    print(f"Sample {i+1}: File Path = {file_paths[i]}, Label = {label} ({class_name})")

# First split: 80% training + validation, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(
    file_paths, labels, test_size=0.2, stratify=labels, random_state=42)
# Second split: 75% training, 25% validation (of the 80% training + validation set)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)

print(f'Total samples: {len(file_paths)}')
print(f'Training samples: {len(X_train)}')
print(f'Validation samples: {len(X_val)}')
print(f'Test samples: {len(X_test)}')

"""### Debugging"""

# Function to print class distribution
def print_class_distribution(labels, set_name):
    unique, counts = np.unique(labels, return_counts=True)
    print(f'\nClass distribution in {set_name}:')
    for u, c in zip(unique, counts):
        print(f'Class {u} ({class_names[u]}): {c} samples')

print_class_distribution(y_train, 'Training Set')
print_class_distribution(y_val, 'Validation Set')
print_class_distribution(y_test, 'Test Set')

"""# Feature Extraction"""

pip install spafe

"""## MFCC"""

import librosa
import numpy as np

def extract_mfcc_librosa(file_path, num_mfcc=13, n_fft=2048, hop_length=512):
    signal, sample_rate = librosa.load(file_path, sr=None)

    # Handle empty and short signals
    if len(signal) == 0:
        print(f"Warning: File {file_path} is empty.")
        return np.zeros((0, num_mfcc * 3))

    # Compute MFCC, delta, and delta-delta
    mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)
    delta_mfccs = librosa.feature.delta(mfccs)
    delta2_mfccs = librosa.feature.delta(mfccs, order=2)
    mfcc_features = np.vstack([mfccs, delta_mfccs, delta2_mfccs])

    # Transpose to shape (time_steps, features)
    mfcc_features = mfcc_features.T

    return mfcc_features

def extract_features(file_paths, num_mfcc=13, maxlen=400):
    features = []
    num_features = num_mfcc * 3
    for idx, file_path in enumerate(file_paths):
        mfcc_features = extract_mfcc_librosa(file_path, num_mfcc=num_mfcc)

        # Verify the number of features
        if mfcc_features.shape[1] != num_features:
            print(f"Warning: Feature dimension mismatch in file {file_path}. Expected {num_features}, got {mfcc_features.shape[1]}")
            # Truncating or Padding feature dimension
            if mfcc_features.shape[1] > num_features:
                mfcc_features = mfcc_features[:, :num_features]
            else:
                padding = np.zeros((mfcc_features.shape[0], num_features - mfcc_features.shape[1]))
                mfcc_features = np.hstack((mfcc_features, padding))

        mfcc_features_padded = pad_or_truncate(mfcc_features, maxlen)
        print(f'File {idx}: Feature shape: {mfcc_features_padded.shape}')

        features.append(mfcc_features_padded)
    return np.array(features)

"""## GFCC"""

import librosa
import numpy as np
from spafe.features.gfcc import gfcc

def extract_gfcc_spafe(file_path, num_gfcc=13, n_fft=2048, hop_length=512):
    signal, sample_rate = librosa.load(file_path, sr=None)

    # Handle empty or short signals
    if len(signal) == 0:
        print(f"Warning: File {file_path} is empty.")
        return np.zeros((0, num_gfcc * 3))

    # Compute GFCC, delta, and delta-delta
    gfccs = gfcc(sig=signal, fs=sample_rate, num_ceps=num_gfcc, nfilts=64, nfft=n_fft, low_freq=0, high_freq=sample_rate/2, scale='constant')
    if gfccs.shape[0] != num_gfcc:
        print(f"Warning: GFCCs shape mismatch in file {file_path}. Expected {num_gfcc}, got {gfccs.shape[0]}")
        # Truncate or pad
        if gfccs.shape[0] > num_gfcc:
            gfccs = gfccs[:num_gfcc, :]
        else:
            padding = np.zeros((num_gfcc - gfccs.shape[0], gfccs.shape[1]))
            gfccs = np.vstack((gfccs, padding))
    delta_gfccs = librosa.feature.delta(gfccs)
    delta2_gfccs = librosa.feature.delta(gfccs, order=2)
    gfcc_features = np.vstack([gfccs, delta_gfccs, delta2_gfccs])

    # Transpose to shape (time_steps, features)
    gfcc_features = gfcc_features.T

    return gfcc_features

def extract_features(file_paths, num_gfcc=13, maxlen=400):
    features = []
    num_features = num_gfcc * 3
    for idx, file_path in enumerate(file_paths):
        gfcc_features = extract_gfcc_spafe(file_path, num_gfcc=num_gfcc)

        # Verify the number of features
        if gfcc_features.shape[1] != num_features:
            print(f"Warning: Feature dimension mismatch in file {file_path}. Expected {num_features}, got {gfcc_features.shape[1]}")
            # Truncating or padding the feature dimension
            if gfcc_features.shape[1] > num_features:
                gfcc_features = gfcc_features[:, :num_features]
            else:
                padding = np.zeros((gfcc_features.shape[0], num_features - gfcc_features.shape[1]))
                gfcc_features = np.hstack((gfcc_features, padding))
        gfcc_features_padded = pad_or_truncate(gfcc_features, maxlen)
        print(f'File {idx}: Feature shape: {gfcc_features_padded.shape}')

        features.append(gfcc_features_padded)
    return np.array(features)

"""## BFCC"""

import librosa
import numpy as np
from spafe.features.bfcc import bfcc

def extract_bfcc_spafe(file_path, num_bfcc=13, n_fft=2048, hop_length=512):
    signal, sample_rate = librosa.load(file_path, sr=None)
    if len(signal) == 0:
        print(f"Warning: File {file_path} is empty.")
        return np.zeros((0, num_bfcc * 3))

    # Compute BFCCs,delta, and delta-delta
    bfccs = bfcc(sig=signal, fs=sample_rate, num_ceps=num_bfcc, nfilts=64, nfft=n_fft)
    if bfccs.shape[1] != num_bfcc:
        print(f"Warning: BFCCs shape mismatch in file {file_path}. Expected {num_bfcc}, got {bfccs.shape[1]}")
        # Truncate or pad as necessary
        if bfccs.shape[1] > num_bfcc:
            bfccs = bfccs[:, :num_bfcc]
        else:
            padding = np.zeros((bfccs.shape[0], num_bfcc - bfccs.shape[1]))
            bfccs = np.hstack((bfccs, padding))
    delta_bfccs = librosa.feature.delta(bfccs)
    delta2_bfccs = librosa.feature.delta(bfccs, order=2)

    bfcc_features = np.hstack([bfccs, delta_bfccs, delta2_bfccs])

    # Ensure the feature dimension matches
    num_features = num_bfcc * 3
    if bfcc_features.shape[1] != num_features:
        print(f"Warning: Combined BFCC feature dimension mismatch in file {file_path}. Expected {num_features}, got {bfcc_features.shape[1]}")
        if bfcc_features.shape[1] > num_features:
            bfcc_features = bfcc_features[:, :num_features]
        else:
            padding = np.zeros((bfcc_features.shape[0], num_features - bfcc_features.shape[1]))
            bfcc_features = np.hstack((bfcc_features, padding))

    return bfcc_features

def extract_features(file_paths, num_bfcc=13, maxlen=400):
    features = []
    num_features = num_bfcc * 3
    for idx, file_path in enumerate(file_paths):
        bfcc_features = extract_bfcc_spafe(file_path, num_bfcc=num_bfcc)

        # Verify the number of features
        if bfcc_features.shape[1] != num_features:
            print(f"Warning: Feature dimension mismatch in file {file_path}. Expected {num_features}, got {bfcc_features.shape[1]}")
            # Truncate or Pad the feature dimension
            if bfcc_features.shape[1] > num_features:
                bfcc_features = bfcc_features[:, :num_features]
            else:
                padding = np.zeros((bfcc_features.shape[0], num_features - bfcc_features.shape[1]))
                bfcc_features = np.hstack((bfcc_features, padding))
        bfcc_features_padded = pad_or_truncate(bfcc_features, maxlen)

        print(f'File {idx}: Feature shape: {bfcc_features_padded.shape}')

        features.append(bfcc_features_padded)
    return np.array(features)

"""## LPCC"""

import librosa
import numpy as np
import numpy.linalg as la
from spafe.features.lpc import lpcc
from sklearn.preprocessing import StandardScaler

def extract_lpcc_spafe(file_path, num_lpcc=13, hop_length=512, order=13, regularize=True, epsilon=1e-10, normalize=False):
    signal, sample_rate = librosa.load(file_path, sr=None)

    if len(signal) == 0:
        print(f"Warning: File {file_path} is empty.")
        return None

    # Regularization to avoid singular matrix
    if regularize:
        signal += epsilon * np.random.randn(*signal.shape)

    try:
        # Compute LPCC, Delta, dan Delta-Delta
        lpccs = lpcc(
            sig=signal,
            fs=sample_rate,
            order=order,
        )
    except la.LinAlgError as e:
        print(f"LinAlgError for file {file_path}: {e}")
        print("Skipping this file.")
        return None
    delta_lpccs = librosa.feature.delta(lpccs)
    delta2_lpccs = librosa.feature.delta(lpccs, order=2)
    lpcc_features = np.hstack([lpccs, delta_lpccs, delta2_lpccs])

    # Normalize the features
    if normalize:
        scaler = StandardScaler()
        lpcc_features = scaler.fit_transform(lpcc_features)

    # Ensure the feature dimension matches
    num_features = num_lpcc * 3
    if lpcc_features.shape[1] != num_features:
        print(f"Warning: LPCC feature dimension mismatch in file {file_path}. Expected {num_features}, got {lpcc_features.shape[1]}")
        if lpcc_features.shape[1] > num_features:
            lpcc_features = lpcc_features[:, :num_features]
        else:
            padding = np.zeros((lpcc_features.shape[0], num_features - lpcc_features.shape[1]))
            lpcc_features = np.hstack((lpcc_features, padding))

    return lpcc_features

def extract_features(file_paths, num_lpcc=13, maxlen=400, regularize=True, epsilon=1e-10, normalize=False):
    features = []
    num_features = num_lpcc * 3
    for idx, file_path in enumerate(file_paths):
        lpcc_features = extract_lpcc_spafe(
            file_path,
            regularize=regularize,
            epsilon=epsilon,
            normalize=normalize
        )
        if lpcc_features is None:
            continue  # Skip file
        lpcc_features_padded = pad_or_truncate(lpcc_features, maxlen)
        features.append(lpcc_features_padded)
        print(f'File {idx}: Feature shape: {lpcc_features_padded.shape}')
    return np.array(features)

"""## Pad and Truncate"""

def pad_or_truncate(sequence, maxlen):
    length = sequence.shape[0]
    if length > maxlen:
        # Truncate the sequence
        return sequence[:maxlen, :]
    elif length < maxlen:
        # Pad the sequence with zeros
        padding = np.zeros((maxlen - length, sequence.shape[1]))
        return np.vstack((sequence, padding))
    else:
        # Sequence is already of maxlen
        return sequence

"""## Extract Features"""

# Extract features for the training set
X_train_features = extract_features(X_train, num_gfcc=13, maxlen=400)  # Shape: (num_train_samples, 400, features)

# Compute mean and std over samples and time steps
mean = np.mean(X_train_features, axis=(0, 1))
std = np.std(X_train_features, axis=(0, 1))

np.save('/content/train_mean.npy', mean)
np.save('/content/train_std.npy', std)

# Normalize training features
X_train_features_norm = (X_train_features - mean) / std

# Extract features for the validation set
X_val_features = extract_features(X_val, num_mfcc=13, maxlen=400)
# Normalize validation features
X_val_features_norm = (X_val_features - mean) / std

# Extract features for the test set
X_test_features = extract_features(X_test, num_mfcc=13, maxlen=400)
# Normalize test features
X_test_features_norm = (X_test_features - mean) / std

# Save training data
np.save('/content/X_train_features_norm.npy', X_train_features_norm)
np.save('/content/y_train.npy', y_train)

# Save validation data
np.save('/content/X_val_features_norm.npy', X_val_features_norm)
np.save('/content/y_val.npy', y_val)

# Save test data
np.save('/content/X_test_features_norm.npy', X_test_features_norm)
np.save('/content/y_test.npy', y_test)

print("Features and labels saved successfully.")

"""## Visualization"""

data = np.load('/content/X_train_features_norm.npy')
print(data.shape)

# Load the .npy file
data = np.load('/content/X_train_features_norm.npy')

sample_index = 100
data_sample = data[sample_index]  # Extract shape (400, 39)
data_sample = data_sample.T  # Transpose to (features, time_frames)

plt.figure(figsize=(10, 6))
librosa.display.specshow(data_sample, sr=16000, x_axis='time', cmap='coolwarm')
plt.colorbar(format='%+2.0f dB')
plt.title(f"Sample {sample_index} LPCC Heatmap")
plt.xlabel('Time Frames')
plt.ylabel('MFCC Coefficients')
plt.tight_layout()
plt.show()

"""# Classification"""

from tensorflow.keras import backend as K
K.clear_session()

import numpy as np

# Load training data
X_train_features_norm = np.load('/content/X_train_features_norm.npy')
y_train = np.load('/content/y_train.npy')

# Load validation data
X_val_features_norm = np.load('/content/X_val_features_norm.npy')
y_val = np.load('/content/y_val.npy')

# Load test data
X_test_features_norm = np.load('/content/X_test_features_norm.npy')
y_test = np.load('/content/y_test.npy')

print("Features and labels loaded successfully.")

"""## Data for CNN"""

# Add a channel dimension
X_train_cnn = X_train_features_norm[..., np.newaxis]
X_val_cnn = X_val_features_norm[..., np.newaxis]
X_test_cnn = X_test_features_norm[..., np.newaxis]

print(f'Training data shape for CNN: {X_train_cnn.shape}')

"""## Data for RNN"""

X_train_rnn = X_train_features_norm
X_val_rnn = X_val_features_norm
X_test_rnn = X_test_features_norm
print(f'Training data shape: {X_train_rnn.shape}')
print(f'Training labels shape: {y_train.shape}')

"""## Data for LSTM"""

X_train_lstm = X_train_features_norm
X_val_lstm = X_val_features_norm
X_test_lstm = X_test_features_norm
print(f'Training data shape: {X_train_lstm.shape}')
print(f'Training labels shape: {y_train.shape}')

"""## CNN"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
import numpy as np

# Define CNN model
def create_model(input_shape, num_classes = 5):
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    return model

input_shape = X_train_cnn.shape[1:]
model = create_model(input_shape)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

"""## RNN"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input,
    SimpleRNN,
    Bidirectional,
    Dense,
    Dropout
)

def build_simple_rnn_model(input_shape, num_classes=5):
    inputs = Input(shape=input_shape)  # Shape: (time_steps, num_features)
    x = Bidirectional(SimpleRNN(units=128, return_sequences=True))(inputs)
    x = Dropout(0.5)(x)
    x = Bidirectional(SimpleRNN(units=128, return_sequences=False))(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

input_shape = X_train_rnn.shape[1:]
model = build_simple_rnn_model(input_shape)

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""## LSTM"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input,
    LSTM,
    Bidirectional,
    Dense,
    Dropout
)

def build_lstm_model(input_shape, num_classes=5):
    inputs = Input(shape=input_shape)  # Shape: (time_steps, num_features)

    x = Bidirectional(LSTM(units=128, return_sequences=True))(inputs)
    x = Dropout(0.5)(x)
    x = Bidirectional(LSTM(units=128, return_sequences=False))(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)

    outputs = Dense(num_classes, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=outputs)
    return model

input_shape = X_train_lstm.shape[1:]
model = build_lstm_model(input_shape)

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""## Early Stop"""

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import load_model

# Define Early Stopping
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

# Train model with Early Stopping
history = model.fit(
    X_train_lstm, y_train,
    validation_data=(X_val_lstm, y_val),
    epochs=30,
    batch_size=32,
    callbacks=[early_stopping],
    verbose=1
)

import matplotlib.pyplot as plt

# Create a figure with two subplots side by side
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot accuracy on the first subplot
ax1.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Accuracy')
ax1.set_title('Model Accuracy')
ax1.legend()
ax1.grid(True)

# Plot loss on the second subplot
ax2.plot(history.history['loss'], label='Train Loss', marker='o')
ax2.plot(history.history['val_loss'], label='Validation Loss', marker='o')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Loss')
ax2.set_title('Model Loss')
ax2.legend()
ax2.grid(True)

# Adjust layout to avoid overlap and display the plots
plt.tight_layout()
plt.show()

"""## CNN Model"""

import joblib
model.save('/content/cnnModel_earlyStop.h5')

"""## RNN Model"""

import joblib
model.save('/content/rnnModel_earlyStop.h5')

"""## LSTM Model"""

import joblib
model.save('/content/lstmModel_earlyStop.h5')

"""# Model Evaluation"""

from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from tensorflow.keras.models import load_model
model = load_model('/content/lstmModel_earlyStop.h5')

"""## Classify Test Dataset"""

# Evaluate the model on Test data
test_loss, test_accuracy = model.evaluate(X_test_lstm, y_test)
print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

"""## Generate Predictions"""

# Generate predictions on the Test data
test_predictions = model.predict(X_test_lstm)
test_pred_classes = np.argmax(test_predictions, axis=1)  # Convert probabilities to class indices

num_samples_to_display = 10
class_names = ['Maju', 'Mundur', 'Kanan', 'Kiri', 'Berhenti']

print("\nSample Predictions:\n")
for i in range(num_samples_to_display):
    true_label = y_test[i]
    predicted_label = test_pred_classes[i]
    true_class_name = class_names[true_label]
    predicted_class_name = class_names[predicted_label]
    print(f"Sample {i+1}: True Label = {true_class_name} ({true_label}), Predicted Label = {predicted_class_name} ({predicted_label})")

"""## Classification Report"""

# Classification report
print("\nClassification Report:\n")
print(classification_report(y_test, test_pred_classes,zero_division=0))

"""## Confusion Matrix"""

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, test_pred_classes)

# Class Accuracy
class_names = ['Maju', 'Mundur', 'Kanan', 'Kiri', 'Berhenti']
for i, class_name in enumerate(class_names):  # Replace class_names with your class labels list
    class_accuracy = conf_matrix[i, i] / np.sum(conf_matrix[i])
    print(f"Class: {class_name}")
    print(f" - Accuracy: {class_accuracy:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_val), yticklabels=np.unique(y_val))
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Confusion Matrix')
plt.show()

"""# Pearson Correlation"""

import numpy as np
from scipy.stats import pearsonr

# Classification Data
without_noise_reduction = [0.9654, 0.8262, 0.9545, 0.9154]
with_noise_reduction = [0.9645, 0.8317, 0.8280, 0.9454]

def calculate_pearson_correlation(x, y):
    x_mean = np.mean(x)
    y_mean = np.mean(y)

    numerator = np.sum((x - x_mean) * (y - y_mean))
    denominator = np.sqrt(np.sum((x - x_mean) ** 2) * np.sum((y - y_mean) ** 2))
    correlation = numerator / denominator
    return correlation

pearson_corr_manual = calculate_pearson_correlation(without_noise_reduction, with_noise_reduction)
print("Pearson Correlation:", pearson_corr_manual)

"""# System Test"""

from google.colab import drive
drive.mount('/content/drive')

"""## Configuration Parameter"""

import os
import numpy as np
import librosa
import soundfile as sf
from tensorflow.keras.models import load_model

# Configuration Parameters
TARGET_SR = 16000
NUM_MFCC = 13
MAXLEN = 400
N_FFT = 2048
HOP_LENGTH = 512
CLASS_NAMES = ['Maju', 'Mundur', 'Kanan', 'Kiri', 'Berhenti']

TRAIN_MEAN_PATH = '/content/train_mean.npy'
TRAIN_STD_PATH = '/content/train_std.npy'
MODEL_PATH = '/content/lstmModel_earlyStop.h5'

# Load classification model
model = load_model(MODEL_PATH)
print("Model loaded successfully.")

# Load normalization parameters
mean = np.load(TRAIN_MEAN_PATH)
std = np.load(TRAIN_STD_PATH)
print("Normalization parameters loaded successfully.")

"""## New Audio"""

import os
import numpy as np

# Load New Test Data and Labels
new_audio_files = [
    '/content/Processed Test Maju 1.wav',
    '/content/Processed Test Maju 2.wav',
    '/content/Processed Test Maju 3.wav',
    '/content/Processed Test Maju 4.wav',
    '/content/Processed Test Kanan 1.wav',
    '/content/Processed Test Kiri 1.wav',
    '/content/Processed Test Mundur 1.wav',
    '/content/Processed Test Berhenti 1.wav',
]

new_labels = [
    0,
    0,
    0,
    0,
    2,
    3,
    1,
    4,
]

# Verify all new audio files exist
for file_path in new_audio_files:
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"Audio file not found: {file_path}")

# Convert to NumPy arrays
new_audio_files = np.array(new_audio_files)
new_labels = np.array(new_labels)

# Function to resample an audio file
def resample_audio_file(input_path, target_sr=22050):
    try:
        signal, sr = librosa.load(input_path, sr=None)
        if sr != target_sr:
            signal = librosa.resample(signal, orig_sr=sr, target_sr=target_sr)
            print(f"Resampled from {sr} Hz to {target_sr} Hz.")
        else:
            print(f"No resampling needed for {input_path}.")
        return signal
    except Exception as e:
        print(f"Error resampling {input_path}: {e}")
        return None

"""## Extract Features"""

# Function to extract features
def extract_features(signal, sr=22050, num_mfcc=13, n_fft=2048, hop_length=512, maxlen=400):
    try:
        # Compute MFCCs
        mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)
        delta_mfccs = librosa.feature.delta(mfccs)
        delta2_mfccs = librosa.feature.delta(mfccs, order=2)
        mfcc_features = np.vstack([mfccs, delta_mfccs, delta2_mfccs]).T

        # Pad or truncate to maxlen
        if mfcc_features.shape[0] > maxlen:
            mfcc_features_padded = mfcc_features[:maxlen, :]
        elif mfcc_features.shape[0] < maxlen:
            padding = np.zeros((maxlen - mfcc_features.shape[0], mfcc_features.shape[1]))
            mfcc_features_padded = np.vstack([mfcc_features, padding])
        else:
            mfcc_features_padded = mfcc_features

        return mfcc_features_padded
    except Exception as e:
        print(f"Error extracting features: {e}")
        return np.zeros((maxlen, num_mfcc * 3))

# Process each new audio file: resample, extract features
new_features = []
valid_new_labels = []

for idx, file_path in enumerate(new_audio_files):
    signal = resample_audio_file(file_path, target_sr=TARGET_SR)
    if signal is not None:
        # Extract features
        features = extract_features(
            signal=signal,
            sr=TARGET_SR,
            num_mfcc=NUM_MFCC,
            n_fft=N_FFT,
            hop_length=HOP_LENGTH,
            maxlen=MAXLEN
        )
        new_features.append(features)
        valid_new_labels.append(new_labels[idx])
        print(f"Processed and extracted features")
    else:
        print(f"Skipping feature extraction for {file_path} due to resampling error.")
        new_features.append(np.zeros((MAXLEN, NUM_MFCC * 3)))
        valid_new_labels.append(new_labels[idx])

new_features = np.array(new_features)
valid_new_labels = np.array(valid_new_labels)

print(f"New features shape: {new_features.shape}")
print(f"Valid new labels shape: {valid_new_labels.shape}")

# Normalize new features using training set statistics
new_features_norm = (new_features - mean) / std
print(f"Normalized new features shape: {new_features_norm.shape}")

# Check for zeros in std
if np.any(std == 0):
    raise ValueError("Standard deviation contains zeros, cannot normalize.")

"""## Generate Predictions"""

# Generate predictions
new_test_predictions = model.predict(new_features_norm)
new_test_pred_classes = np.argmax(new_test_predictions, axis=1)  # Convert probabilities to class indices

print("Generated predictions for new test samples.")

import pandas as pd

# Display prediction results
print("New Test Samples Predictions:\n")
for i in range(len(new_audio_files)):
    true_label = new_labels[i]
    predicted_label = new_test_pred_classes[i]
    true_class_name = CLASS_NAMES[true_label] if true_label < len(CLASS_NAMES) else "Unknown"
    predicted_class_name = CLASS_NAMES[predicted_label] if predicted_label < len(CLASS_NAMES) else "Unknown"
    print(f"Sample {i+1}: True Label = {true_class_name} ({true_label}), Predicted Label = {predicted_class_name} ({predicted_label})")
